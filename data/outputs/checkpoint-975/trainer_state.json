{
  "best_metric": 0.19082452356815338,
  "best_model_checkpoint": "./data/outputs/checkpoint-975",
  "epoch": 5.0,
  "eval_steps": 500,
  "global_step": 975,
  "is_hyper_param_search": false,
  "is_local_process_zero": true,
  "is_world_process_zero": true,
  "log_history": [
    {
      "epoch": 0.02564102564102564,
      "grad_norm": 0.3746504783630371,
      "learning_rate": 0.0049743589743589745,
      "loss": 0.6266,
      "step": 5
    },
    {
      "epoch": 0.05128205128205128,
      "grad_norm": 1.416567325592041,
      "learning_rate": 0.004948717948717949,
      "loss": 0.4624,
      "step": 10
    },
    {
      "epoch": 0.07692307692307693,
      "grad_norm": 0.4465400278568268,
      "learning_rate": 0.004923076923076923,
      "loss": 0.5689,
      "step": 15
    },
    {
      "epoch": 0.10256410256410256,
      "grad_norm": 1.5187485218048096,
      "learning_rate": 0.004897435897435898,
      "loss": 0.4456,
      "step": 20
    },
    {
      "epoch": 0.1282051282051282,
      "grad_norm": 1.1969140768051147,
      "learning_rate": 0.004871794871794872,
      "loss": 0.3142,
      "step": 25
    },
    {
      "epoch": 0.15384615384615385,
      "grad_norm": 0.8241394758224487,
      "learning_rate": 0.004846153846153846,
      "loss": 0.5339,
      "step": 30
    },
    {
      "epoch": 0.1794871794871795,
      "grad_norm": 0.6593533754348755,
      "learning_rate": 0.004820512820512821,
      "loss": 0.2213,
      "step": 35
    },
    {
      "epoch": 0.20512820512820512,
      "grad_norm": 0.30296623706817627,
      "learning_rate": 0.004794871794871795,
      "loss": 0.3832,
      "step": 40
    },
    {
      "epoch": 0.23076923076923078,
      "grad_norm": 1.384635329246521,
      "learning_rate": 0.0047692307692307695,
      "loss": 0.4892,
      "step": 45
    },
    {
      "epoch": 0.2564102564102564,
      "grad_norm": NaN,
      "learning_rate": 0.0047538461538461545,
      "loss": 0.8312,
      "step": 50
    },
    {
      "epoch": 0.28205128205128205,
      "grad_norm": 1.5331690311431885,
      "learning_rate": 0.004738461538461539,
      "loss": 0.9973,
      "step": 55
    },
    {
      "epoch": 0.3076923076923077,
      "grad_norm": 2.253283739089966,
      "learning_rate": 0.004712820512820513,
      "loss": 0.3293,
      "step": 60
    },
    {
      "epoch": 0.3333333333333333,
      "grad_norm": 0.7175338864326477,
      "learning_rate": 0.004687179487179487,
      "loss": 0.4343,
      "step": 65
    },
    {
      "epoch": 0.358974358974359,
      "grad_norm": 0.9708196520805359,
      "learning_rate": 0.004661538461538462,
      "loss": 0.7062,
      "step": 70
    },
    {
      "epoch": 0.38461538461538464,
      "grad_norm": 1.018166184425354,
      "learning_rate": 0.004635897435897436,
      "loss": 0.4032,
      "step": 75
    },
    {
      "epoch": 0.41025641025641024,
      "grad_norm": 1.4357560873031616,
      "learning_rate": 0.0046102564102564105,
      "loss": 0.4391,
      "step": 80
    },
    {
      "epoch": 0.4358974358974359,
      "grad_norm": 0.771258533000946,
      "learning_rate": 0.004584615384615385,
      "loss": 0.2994,
      "step": 85
    },
    {
      "epoch": 0.46153846153846156,
      "grad_norm": 0.6537236571311951,
      "learning_rate": 0.004558974358974359,
      "loss": 0.4906,
      "step": 90
    },
    {
      "epoch": 0.48717948717948717,
      "grad_norm": 0.23271457850933075,
      "learning_rate": 0.004533333333333333,
      "loss": 0.1684,
      "step": 95
    },
    {
      "epoch": 0.5128205128205128,
      "grad_norm": 0.366607666015625,
      "learning_rate": 0.004507692307692308,
      "loss": 0.5562,
      "step": 100
    },
    {
      "epoch": 0.5384615384615384,
      "grad_norm": 0.4409780204296112,
      "learning_rate": 0.004482051282051282,
      "loss": 0.361,
      "step": 105
    },
    {
      "epoch": 0.5641025641025641,
      "grad_norm": 0.6698353886604309,
      "learning_rate": 0.004456410256410257,
      "loss": 0.1688,
      "step": 110
    },
    {
      "epoch": 0.5897435897435898,
      "grad_norm": 3.4770989418029785,
      "learning_rate": 0.004430769230769231,
      "loss": 0.6601,
      "step": 115
    },
    {
      "epoch": 0.6153846153846154,
      "grad_norm": 1.2290056943893433,
      "learning_rate": 0.0044051282051282056,
      "loss": 0.4822,
      "step": 120
    },
    {
      "epoch": 0.6410256410256411,
      "grad_norm": 0.2543763518333435,
      "learning_rate": 0.004379487179487179,
      "loss": 0.3409,
      "step": 125
    },
    {
      "epoch": 0.6666666666666666,
      "grad_norm": 1.5806244611740112,
      "learning_rate": 0.0043538461538461535,
      "loss": 0.2827,
      "step": 130
    },
    {
      "epoch": 0.6923076923076923,
      "grad_norm": 0.5473930835723877,
      "learning_rate": 0.004328205128205128,
      "loss": 0.5744,
      "step": 135
    },
    {
      "epoch": 0.717948717948718,
      "grad_norm": 2.7770161628723145,
      "learning_rate": 0.004302564102564103,
      "loss": 0.3894,
      "step": 140
    },
    {
      "epoch": 0.7435897435897436,
      "grad_norm": 0.7162584066390991,
      "learning_rate": 0.0042769230769230775,
      "loss": 0.5762,
      "step": 145
    },
    {
      "epoch": 0.7692307692307693,
      "grad_norm": 1.3476965427398682,
      "learning_rate": 0.004251282051282052,
      "loss": 0.6291,
      "step": 150
    },
    {
      "epoch": 0.7948717948717948,
      "grad_norm": 0.6831963062286377,
      "learning_rate": 0.004225641025641025,
      "loss": 0.1581,
      "step": 155
    },
    {
      "epoch": 0.8205128205128205,
      "grad_norm": 0.4423077702522278,
      "learning_rate": 0.0042,
      "loss": 0.2503,
      "step": 160
    },
    {
      "epoch": 0.8461538461538461,
      "grad_norm": 0.4234243631362915,
      "learning_rate": 0.004174358974358974,
      "loss": 0.6508,
      "step": 165
    },
    {
      "epoch": 0.8717948717948718,
      "grad_norm": 0.41049879789352417,
      "learning_rate": 0.004153846153846154,
      "loss": 0.2031,
      "step": 170
    },
    {
      "epoch": 0.8974358974358975,
      "grad_norm": 2.4943137168884277,
      "learning_rate": 0.004128205128205128,
      "loss": 0.3206,
      "step": 175
    },
    {
      "epoch": 0.9230769230769231,
      "grad_norm": 1.0996671915054321,
      "learning_rate": 0.0041025641025641026,
      "loss": 0.7345,
      "step": 180
    },
    {
      "epoch": 0.9487179487179487,
      "grad_norm": 1.065631628036499,
      "learning_rate": 0.004076923076923077,
      "loss": 0.385,
      "step": 185
    },
    {
      "epoch": 0.9743589743589743,
      "grad_norm": 0.46617335081100464,
      "learning_rate": 0.004051282051282051,
      "loss": 0.5838,
      "step": 190
    },
    {
      "epoch": 1.0,
      "grad_norm": 0.75684654712677,
      "learning_rate": 0.004025641025641026,
      "loss": 0.652,
      "step": 195
    },
    {
      "epoch": 1.0,
      "eval_loss": 0.2955986559391022,
      "eval_runtime": 0.9576,
      "eval_samples_per_second": 45.95,
      "eval_steps_per_second": 45.95,
      "step": 195
    },
    {
      "epoch": 1.0256410256410255,
      "grad_norm": 0.5663767457008362,
      "learning_rate": 0.004,
      "loss": 0.2213,
      "step": 200
    },
    {
      "epoch": 1.0512820512820513,
      "grad_norm": 1.3986014127731323,
      "learning_rate": 0.0039743589743589745,
      "loss": 0.2498,
      "step": 205
    },
    {
      "epoch": 1.0769230769230769,
      "grad_norm": 0.5597972869873047,
      "learning_rate": 0.003948717948717949,
      "loss": 0.3614,
      "step": 210
    },
    {
      "epoch": 1.1025641025641026,
      "grad_norm": 0.7320801019668579,
      "learning_rate": 0.003923076923076923,
      "loss": 0.256,
      "step": 215
    },
    {
      "epoch": 1.1282051282051282,
      "grad_norm": 1.3783762454986572,
      "learning_rate": 0.0038974358974358976,
      "loss": 0.3613,
      "step": 220
    },
    {
      "epoch": 1.1538461538461537,
      "grad_norm": 0.6245347857475281,
      "learning_rate": 0.003871794871794872,
      "loss": 0.3229,
      "step": 225
    },
    {
      "epoch": 1.1794871794871795,
      "grad_norm": 0.8274028897285461,
      "learning_rate": 0.0038461538461538464,
      "loss": 0.257,
      "step": 230
    },
    {
      "epoch": 1.205128205128205,
      "grad_norm": 0.25290513038635254,
      "learning_rate": 0.0038205128205128203,
      "loss": 0.4007,
      "step": 235
    },
    {
      "epoch": 1.2307692307692308,
      "grad_norm": 0.5485237836837769,
      "learning_rate": 0.0037948717948717947,
      "loss": 0.464,
      "step": 240
    },
    {
      "epoch": 1.2564102564102564,
      "grad_norm": 0.5704793334007263,
      "learning_rate": 0.003769230769230769,
      "loss": 0.3365,
      "step": 245
    },
    {
      "epoch": 1.282051282051282,
      "grad_norm": 0.32877492904663086,
      "learning_rate": 0.003743589743589744,
      "loss": 0.3916,
      "step": 250
    },
    {
      "epoch": 1.3076923076923077,
      "grad_norm": 0.273531973361969,
      "learning_rate": 0.0037179487179487183,
      "loss": 0.1895,
      "step": 255
    },
    {
      "epoch": 1.3333333333333333,
      "grad_norm": 0.5087133646011353,
      "learning_rate": 0.0036923076923076927,
      "loss": 0.2927,
      "step": 260
    },
    {
      "epoch": 1.358974358974359,
      "grad_norm": 0.2906315326690674,
      "learning_rate": 0.0036666666666666666,
      "loss": 0.3476,
      "step": 265
    },
    {
      "epoch": 1.3846153846153846,
      "grad_norm": 0.41664284467697144,
      "learning_rate": 0.003641025641025641,
      "loss": 0.3527,
      "step": 270
    },
    {
      "epoch": 1.4102564102564101,
      "grad_norm": 0.1781402826309204,
      "learning_rate": 0.0036153846153846154,
      "loss": 0.2021,
      "step": 275
    },
    {
      "epoch": 1.435897435897436,
      "grad_norm": 0.3172791004180908,
      "learning_rate": 0.0035897435897435897,
      "loss": 0.4582,
      "step": 280
    },
    {
      "epoch": 1.4615384615384617,
      "grad_norm": 0.19524691998958588,
      "learning_rate": 0.003564102564102564,
      "loss": 0.2514,
      "step": 285
    },
    {
      "epoch": 1.4871794871794872,
      "grad_norm": 0.9378566741943359,
      "learning_rate": 0.003538461538461539,
      "loss": 0.3967,
      "step": 290
    },
    {
      "epoch": 1.5128205128205128,
      "grad_norm": 0.42095324397087097,
      "learning_rate": 0.0035128205128205124,
      "loss": 0.3063,
      "step": 295
    },
    {
      "epoch": 1.5384615384615383,
      "grad_norm": 0.2986108958721161,
      "learning_rate": 0.0034871794871794873,
      "loss": 0.2591,
      "step": 300
    },
    {
      "epoch": 1.564102564102564,
      "grad_norm": 0.2929207682609558,
      "learning_rate": 0.0034615384615384616,
      "loss": 0.1012,
      "step": 305
    },
    {
      "epoch": 1.5897435897435899,
      "grad_norm": 0.592534065246582,
      "learning_rate": 0.003435897435897436,
      "loss": 0.3333,
      "step": 310
    },
    {
      "epoch": 1.6153846153846154,
      "grad_norm": 1.1234285831451416,
      "learning_rate": 0.0034102564102564104,
      "loss": 0.3236,
      "step": 315
    },
    {
      "epoch": 1.641025641025641,
      "grad_norm": 0.24362316727638245,
      "learning_rate": 0.003384615384615385,
      "loss": 0.2837,
      "step": 320
    },
    {
      "epoch": 1.6666666666666665,
      "grad_norm": 0.7240253686904907,
      "learning_rate": 0.0033589743589743587,
      "loss": 0.227,
      "step": 325
    },
    {
      "epoch": 1.6923076923076923,
      "grad_norm": 0.28621968626976013,
      "learning_rate": 0.003333333333333333,
      "loss": 0.3196,
      "step": 330
    },
    {
      "epoch": 1.717948717948718,
      "grad_norm": 0.7623562812805176,
      "learning_rate": 0.0033076923076923075,
      "loss": 0.4971,
      "step": 335
    },
    {
      "epoch": 1.7435897435897436,
      "grad_norm": 0.2411782443523407,
      "learning_rate": 0.0032820512820512823,
      "loss": 0.2428,
      "step": 340
    },
    {
      "epoch": 1.7692307692307692,
      "grad_norm": 0.5108322501182556,
      "learning_rate": 0.0032564102564102567,
      "loss": 0.6695,
      "step": 345
    },
    {
      "epoch": 1.7948717948717947,
      "grad_norm": 0.5695626735687256,
      "learning_rate": 0.003230769230769231,
      "loss": 0.1496,
      "step": 350
    },
    {
      "epoch": 1.8205128205128205,
      "grad_norm": 0.2129751592874527,
      "learning_rate": 0.0032051282051282055,
      "loss": 0.1569,
      "step": 355
    },
    {
      "epoch": 1.8461538461538463,
      "grad_norm": 0.8402166366577148,
      "learning_rate": 0.0031794871794871794,
      "loss": 0.6013,
      "step": 360
    },
    {
      "epoch": 1.8717948717948718,
      "grad_norm": 0.36945390701293945,
      "learning_rate": 0.0031538461538461538,
      "loss": 0.6215,
      "step": 365
    },
    {
      "epoch": 1.8974358974358974,
      "grad_norm": 0.14191995561122894,
      "learning_rate": 0.003128205128205128,
      "loss": 0.2797,
      "step": 370
    },
    {
      "epoch": 1.9230769230769231,
      "grad_norm": 0.4011377990245819,
      "learning_rate": 0.0031025641025641025,
      "loss": 0.2469,
      "step": 375
    },
    {
      "epoch": 1.9487179487179487,
      "grad_norm": 0.1276296228170395,
      "learning_rate": 0.0030769230769230774,
      "loss": 0.1473,
      "step": 380
    },
    {
      "epoch": 1.9743589743589745,
      "grad_norm": 1.1236311197280884,
      "learning_rate": 0.0030512820512820517,
      "loss": 0.3233,
      "step": 385
    },
    {
      "epoch": 2.0,
      "grad_norm": 0.27718427777290344,
      "learning_rate": 0.0030256410256410257,
      "loss": 0.3266,
      "step": 390
    },
    {
      "epoch": 2.0,
      "eval_loss": 0.26747626066207886,
      "eval_runtime": 0.9476,
      "eval_samples_per_second": 46.433,
      "eval_steps_per_second": 46.433,
      "step": 390
    },
    {
      "epoch": 2.0256410256410255,
      "grad_norm": 0.6482958197593689,
      "learning_rate": 0.003,
      "loss": 0.3747,
      "step": 395
    },
    {
      "epoch": 2.051282051282051,
      "grad_norm": 0.24573026597499847,
      "learning_rate": 0.0029743589743589744,
      "loss": 0.2983,
      "step": 400
    },
    {
      "epoch": 2.076923076923077,
      "grad_norm": 0.6918169856071472,
      "learning_rate": 0.002948717948717949,
      "loss": 0.2231,
      "step": 405
    },
    {
      "epoch": 2.1025641025641026,
      "grad_norm": 0.10757862031459808,
      "learning_rate": 0.002923076923076923,
      "loss": 0.1864,
      "step": 410
    },
    {
      "epoch": 2.128205128205128,
      "grad_norm": 0.20813871920108795,
      "learning_rate": 0.0028974358974358976,
      "loss": 0.1837,
      "step": 415
    },
    {
      "epoch": 2.1538461538461537,
      "grad_norm": 0.15110354125499725,
      "learning_rate": 0.0028717948717948715,
      "loss": 0.128,
      "step": 420
    },
    {
      "epoch": 2.1794871794871793,
      "grad_norm": 0.6339870691299438,
      "learning_rate": 0.002846153846153846,
      "loss": 0.2232,
      "step": 425
    },
    {
      "epoch": 2.2051282051282053,
      "grad_norm": 0.2479148805141449,
      "learning_rate": 0.0028205128205128207,
      "loss": 0.3392,
      "step": 430
    },
    {
      "epoch": 2.230769230769231,
      "grad_norm": 0.23380829393863678,
      "learning_rate": 0.002794871794871795,
      "loss": 0.2445,
      "step": 435
    },
    {
      "epoch": 2.2564102564102564,
      "grad_norm": 0.11068897694349289,
      "learning_rate": 0.0027692307692307695,
      "loss": 0.2517,
      "step": 440
    },
    {
      "epoch": 2.282051282051282,
      "grad_norm": 0.6633663177490234,
      "learning_rate": 0.002743589743589744,
      "loss": 0.3687,
      "step": 445
    },
    {
      "epoch": 2.3076923076923075,
      "grad_norm": 0.1695525050163269,
      "learning_rate": 0.002717948717948718,
      "loss": 0.3055,
      "step": 450
    },
    {
      "epoch": 2.3333333333333335,
      "grad_norm": 0.15709589421749115,
      "learning_rate": 0.002692307692307692,
      "loss": 0.1905,
      "step": 455
    },
    {
      "epoch": 2.358974358974359,
      "grad_norm": 0.13948793709278107,
      "learning_rate": 0.0026666666666666666,
      "loss": 0.2319,
      "step": 460
    },
    {
      "epoch": 2.3846153846153846,
      "grad_norm": 0.19037552177906036,
      "learning_rate": 0.002641025641025641,
      "loss": 0.1423,
      "step": 465
    },
    {
      "epoch": 2.41025641025641,
      "grad_norm": 0.14021332561969757,
      "learning_rate": 0.0026153846153846158,
      "loss": 0.3067,
      "step": 470
    },
    {
      "epoch": 2.435897435897436,
      "grad_norm": 0.1747765839099884,
      "learning_rate": 0.00258974358974359,
      "loss": 0.1533,
      "step": 475
    },
    {
      "epoch": 2.4615384615384617,
      "grad_norm": 0.18455329537391663,
      "learning_rate": 0.002564102564102564,
      "loss": 0.3181,
      "step": 480
    },
    {
      "epoch": 2.4871794871794872,
      "grad_norm": 0.3604477643966675,
      "learning_rate": 0.0025384615384615385,
      "loss": 0.2652,
      "step": 485
    },
    {
      "epoch": 2.5128205128205128,
      "grad_norm": 0.37859204411506653,
      "learning_rate": 0.002512820512820513,
      "loss": 0.1049,
      "step": 490
    },
    {
      "epoch": 2.5384615384615383,
      "grad_norm": 0.46990934014320374,
      "learning_rate": 0.0024871794871794872,
      "loss": 0.225,
      "step": 495
    },
    {
      "epoch": 2.564102564102564,
      "grad_norm": 0.447049617767334,
      "learning_rate": 0.0024615384615384616,
      "loss": 0.3743,
      "step": 500
    },
    {
      "epoch": 2.58974358974359,
      "grad_norm": 0.1253635734319687,
      "learning_rate": 0.002435897435897436,
      "loss": 0.1573,
      "step": 505
    },
    {
      "epoch": 2.6153846153846154,
      "grad_norm": 0.46084606647491455,
      "learning_rate": 0.0024102564102564104,
      "loss": 0.1763,
      "step": 510
    },
    {
      "epoch": 2.641025641025641,
      "grad_norm": 0.07201995700597763,
      "learning_rate": 0.0023846153846153848,
      "loss": 0.1392,
      "step": 515
    },
    {
      "epoch": 2.6666666666666665,
      "grad_norm": 0.38091251254081726,
      "learning_rate": 0.002358974358974359,
      "loss": 0.1508,
      "step": 520
    },
    {
      "epoch": 2.6923076923076925,
      "grad_norm": 0.13237302005290985,
      "learning_rate": 0.0023333333333333335,
      "loss": 0.1865,
      "step": 525
    },
    {
      "epoch": 2.717948717948718,
      "grad_norm": 0.20935745537281036,
      "learning_rate": 0.002307692307692308,
      "loss": 0.1738,
      "step": 530
    },
    {
      "epoch": 2.7435897435897436,
      "grad_norm": 0.13400912284851074,
      "learning_rate": 0.002282051282051282,
      "loss": 0.1102,
      "step": 535
    },
    {
      "epoch": 2.769230769230769,
      "grad_norm": 0.19188429415225983,
      "learning_rate": 0.0022564102564102567,
      "loss": 0.1135,
      "step": 540
    },
    {
      "epoch": 2.7948717948717947,
      "grad_norm": 0.6082016825675964,
      "learning_rate": 0.002230769230769231,
      "loss": 0.1762,
      "step": 545
    },
    {
      "epoch": 2.8205128205128203,
      "grad_norm": 0.28857842087745667,
      "learning_rate": 0.002205128205128205,
      "loss": 0.1416,
      "step": 550
    },
    {
      "epoch": 2.8461538461538463,
      "grad_norm": 0.2168489396572113,
      "learning_rate": 0.0021794871794871794,
      "loss": 0.2008,
      "step": 555
    },
    {
      "epoch": 2.871794871794872,
      "grad_norm": 0.5730985999107361,
      "learning_rate": 0.002153846153846154,
      "loss": 0.5853,
      "step": 560
    },
    {
      "epoch": 2.8974358974358974,
      "grad_norm": 0.18958783149719238,
      "learning_rate": 0.002128205128205128,
      "loss": 0.1871,
      "step": 565
    },
    {
      "epoch": 2.9230769230769234,
      "grad_norm": 0.09854351729154587,
      "learning_rate": 0.0021025641025641025,
      "loss": 0.2131,
      "step": 570
    },
    {
      "epoch": 2.948717948717949,
      "grad_norm": 1.0226022005081177,
      "learning_rate": 0.002076923076923077,
      "loss": 0.317,
      "step": 575
    },
    {
      "epoch": 2.9743589743589745,
      "grad_norm": 0.19395588338375092,
      "learning_rate": 0.0020512820512820513,
      "loss": 0.1241,
      "step": 580
    },
    {
      "epoch": 3.0,
      "grad_norm": 0.14145943522453308,
      "learning_rate": 0.0020256410256410257,
      "loss": 0.1133,
      "step": 585
    },
    {
      "epoch": 3.0,
      "eval_loss": 0.20835019648075104,
      "eval_runtime": 0.9557,
      "eval_samples_per_second": 46.038,
      "eval_steps_per_second": 46.038,
      "step": 585
    },
    {
      "epoch": 3.0256410256410255,
      "grad_norm": 0.18800030648708344,
      "learning_rate": 0.002,
      "loss": 0.113,
      "step": 590
    },
    {
      "epoch": 3.051282051282051,
      "grad_norm": 0.21808448433876038,
      "learning_rate": 0.0019743589743589744,
      "loss": 0.2646,
      "step": 595
    },
    {
      "epoch": 3.076923076923077,
      "grad_norm": 0.3425505757331848,
      "learning_rate": 0.0019487179487179488,
      "loss": 0.1621,
      "step": 600
    },
    {
      "epoch": 3.1025641025641026,
      "grad_norm": 0.21368730068206787,
      "learning_rate": 0.0019230769230769232,
      "loss": 0.2172,
      "step": 605
    },
    {
      "epoch": 3.128205128205128,
      "grad_norm": 0.10458770394325256,
      "learning_rate": 0.0018974358974358973,
      "loss": 0.3727,
      "step": 610
    },
    {
      "epoch": 3.1538461538461537,
      "grad_norm": 0.10610389709472656,
      "learning_rate": 0.001871794871794872,
      "loss": 0.1477,
      "step": 615
    },
    {
      "epoch": 3.1794871794871793,
      "grad_norm": 0.1494094878435135,
      "learning_rate": 0.0018461538461538463,
      "loss": 0.1968,
      "step": 620
    },
    {
      "epoch": 3.2051282051282053,
      "grad_norm": 0.15284906327724457,
      "learning_rate": 0.0018205128205128205,
      "loss": 0.2339,
      "step": 625
    },
    {
      "epoch": 3.230769230769231,
      "grad_norm": 0.09184594452381134,
      "learning_rate": 0.0017948717948717949,
      "loss": 0.1014,
      "step": 630
    },
    {
      "epoch": 3.2564102564102564,
      "grad_norm": 0.13354158401489258,
      "learning_rate": 0.0017692307692307695,
      "loss": 0.1496,
      "step": 635
    },
    {
      "epoch": 3.282051282051282,
      "grad_norm": 0.15847893059253693,
      "learning_rate": 0.0017435897435897436,
      "loss": 0.0869,
      "step": 640
    },
    {
      "epoch": 3.3076923076923075,
      "grad_norm": 0.12212512642145157,
      "learning_rate": 0.001717948717948718,
      "loss": 0.2067,
      "step": 645
    },
    {
      "epoch": 3.3333333333333335,
      "grad_norm": 0.7515485286712646,
      "learning_rate": 0.0016923076923076924,
      "loss": 0.1845,
      "step": 650
    },
    {
      "epoch": 3.358974358974359,
      "grad_norm": 0.5016032457351685,
      "learning_rate": 0.0016666666666666666,
      "loss": 0.1458,
      "step": 655
    },
    {
      "epoch": 3.3846153846153846,
      "grad_norm": 0.11824875324964523,
      "learning_rate": 0.0016410256410256412,
      "loss": 0.1117,
      "step": 660
    },
    {
      "epoch": 3.41025641025641,
      "grad_norm": 0.49315178394317627,
      "learning_rate": 0.0016153846153846155,
      "loss": 0.1561,
      "step": 665
    },
    {
      "epoch": 3.435897435897436,
      "grad_norm": 0.33982524275779724,
      "learning_rate": 0.0015897435897435897,
      "loss": 0.1836,
      "step": 670
    },
    {
      "epoch": 3.4615384615384617,
      "grad_norm": 0.40161776542663574,
      "learning_rate": 0.001564102564102564,
      "loss": 0.1667,
      "step": 675
    },
    {
      "epoch": 3.4871794871794872,
      "grad_norm": 0.12153703719377518,
      "learning_rate": 0.0015384615384615387,
      "loss": 0.1447,
      "step": 680
    },
    {
      "epoch": 3.5128205128205128,
      "grad_norm": 0.10700628906488419,
      "learning_rate": 0.0015128205128205128,
      "loss": 0.0708,
      "step": 685
    },
    {
      "epoch": 3.5384615384615383,
      "grad_norm": 0.36501142382621765,
      "learning_rate": 0.0014871794871794872,
      "loss": 0.1033,
      "step": 690
    },
    {
      "epoch": 3.564102564102564,
      "grad_norm": 0.5379574298858643,
      "learning_rate": 0.0014615384615384616,
      "loss": 0.1347,
      "step": 695
    },
    {
      "epoch": 3.58974358974359,
      "grad_norm": 0.12640191614627838,
      "learning_rate": 0.0014358974358974358,
      "loss": 0.1068,
      "step": 700
    },
    {
      "epoch": 3.6153846153846154,
      "grad_norm": 0.35401633381843567,
      "learning_rate": 0.0014102564102564104,
      "loss": 0.0569,
      "step": 705
    },
    {
      "epoch": 3.641025641025641,
      "grad_norm": 0.374615877866745,
      "learning_rate": 0.0013846153846153847,
      "loss": 0.1941,
      "step": 710
    },
    {
      "epoch": 3.6666666666666665,
      "grad_norm": 0.0584743358194828,
      "learning_rate": 0.001358974358974359,
      "loss": 0.1575,
      "step": 715
    },
    {
      "epoch": 3.6923076923076925,
      "grad_norm": 0.12943726778030396,
      "learning_rate": 0.0013333333333333333,
      "loss": 0.1052,
      "step": 720
    },
    {
      "epoch": 3.717948717948718,
      "grad_norm": 0.2006220519542694,
      "learning_rate": 0.0013076923076923079,
      "loss": 0.1326,
      "step": 725
    },
    {
      "epoch": 3.7435897435897436,
      "grad_norm": 0.36663818359375,
      "learning_rate": 0.001282051282051282,
      "loss": 0.1836,
      "step": 730
    },
    {
      "epoch": 3.769230769230769,
      "grad_norm": 0.1208038255572319,
      "learning_rate": 0.0012564102564102564,
      "loss": 0.1382,
      "step": 735
    },
    {
      "epoch": 3.7948717948717947,
      "grad_norm": 0.19243602454662323,
      "learning_rate": 0.0012307692307692308,
      "loss": 0.1945,
      "step": 740
    },
    {
      "epoch": 3.8205128205128203,
      "grad_norm": 0.07966450601816177,
      "learning_rate": 0.0012051282051282052,
      "loss": 0.2007,
      "step": 745
    },
    {
      "epoch": 3.8461538461538463,
      "grad_norm": 0.15473459661006927,
      "learning_rate": 0.0011794871794871796,
      "loss": 0.1933,
      "step": 750
    },
    {
      "epoch": 3.871794871794872,
      "grad_norm": 0.5172169804573059,
      "learning_rate": 0.001153846153846154,
      "loss": 0.1644,
      "step": 755
    },
    {
      "epoch": 3.8974358974358974,
      "grad_norm": 0.18708579242229462,
      "learning_rate": 0.0011282051282051283,
      "loss": 0.1573,
      "step": 760
    },
    {
      "epoch": 3.9230769230769234,
      "grad_norm": 0.17050230503082275,
      "learning_rate": 0.0011025641025641025,
      "loss": 0.2456,
      "step": 765
    },
    {
      "epoch": 3.948717948717949,
      "grad_norm": 0.25708138942718506,
      "learning_rate": 0.001076923076923077,
      "loss": 0.1017,
      "step": 770
    },
    {
      "epoch": 3.9743589743589745,
      "grad_norm": 0.15962770581245422,
      "learning_rate": 0.0010512820512820513,
      "loss": 0.1692,
      "step": 775
    },
    {
      "epoch": 4.0,
      "grad_norm": 0.08107788860797882,
      "learning_rate": 0.0010256410256410256,
      "loss": 0.0649,
      "step": 780
    },
    {
      "epoch": 4.0,
      "eval_loss": 0.19449014961719513,
      "eval_runtime": 0.9534,
      "eval_samples_per_second": 46.151,
      "eval_steps_per_second": 46.151,
      "step": 780
    },
    {
      "epoch": 4.0256410256410255,
      "grad_norm": 0.3897460699081421,
      "learning_rate": 0.001,
      "loss": 0.1011,
      "step": 785
    },
    {
      "epoch": 4.051282051282051,
      "grad_norm": 0.112159363925457,
      "learning_rate": 0.0009743589743589744,
      "loss": 0.1028,
      "step": 790
    },
    {
      "epoch": 4.076923076923077,
      "grad_norm": 0.05670478940010071,
      "learning_rate": 0.0009487179487179487,
      "loss": 0.0491,
      "step": 795
    },
    {
      "epoch": 4.102564102564102,
      "grad_norm": 0.12189363688230515,
      "learning_rate": 0.0009230769230769232,
      "loss": 0.0778,
      "step": 800
    },
    {
      "epoch": 4.128205128205128,
      "grad_norm": 0.07141604274511337,
      "learning_rate": 0.0008974358974358974,
      "loss": 0.153,
      "step": 805
    },
    {
      "epoch": 4.153846153846154,
      "grad_norm": 0.24076439440250397,
      "learning_rate": 0.0008717948717948718,
      "loss": 0.1221,
      "step": 810
    },
    {
      "epoch": 4.17948717948718,
      "grad_norm": 0.4220195412635803,
      "learning_rate": 0.0008461538461538462,
      "loss": 0.1122,
      "step": 815
    },
    {
      "epoch": 4.205128205128205,
      "grad_norm": 0.06425286829471588,
      "learning_rate": 0.0008205128205128206,
      "loss": 0.1011,
      "step": 820
    },
    {
      "epoch": 4.230769230769231,
      "grad_norm": 0.09612339735031128,
      "learning_rate": 0.0007948717948717948,
      "loss": 0.0939,
      "step": 825
    },
    {
      "epoch": 4.256410256410256,
      "grad_norm": 0.32355794310569763,
      "learning_rate": 0.0007692307692307693,
      "loss": 0.0887,
      "step": 830
    },
    {
      "epoch": 4.282051282051282,
      "grad_norm": 0.1958872675895691,
      "learning_rate": 0.0007435897435897436,
      "loss": 0.1017,
      "step": 835
    },
    {
      "epoch": 4.3076923076923075,
      "grad_norm": 0.20586363971233368,
      "learning_rate": 0.0007179487179487179,
      "loss": 0.1219,
      "step": 840
    },
    {
      "epoch": 4.333333333333333,
      "grad_norm": 0.12759484350681305,
      "learning_rate": 0.0006923076923076924,
      "loss": 0.1149,
      "step": 845
    },
    {
      "epoch": 4.358974358974359,
      "grad_norm": 0.8310025334358215,
      "learning_rate": 0.0006666666666666666,
      "loss": 0.2321,
      "step": 850
    },
    {
      "epoch": 4.384615384615385,
      "grad_norm": 0.08601287752389908,
      "learning_rate": 0.000641025641025641,
      "loss": 0.0135,
      "step": 855
    },
    {
      "epoch": 4.410256410256411,
      "grad_norm": 0.25147390365600586,
      "learning_rate": 0.0006153846153846154,
      "loss": 0.1116,
      "step": 860
    },
    {
      "epoch": 4.435897435897436,
      "grad_norm": 0.04128563776612282,
      "learning_rate": 0.0005897435897435898,
      "loss": 0.118,
      "step": 865
    },
    {
      "epoch": 4.461538461538462,
      "grad_norm": 0.12250782549381256,
      "learning_rate": 0.0005641025641025642,
      "loss": 0.0756,
      "step": 870
    },
    {
      "epoch": 4.487179487179487,
      "grad_norm": 0.3469920754432678,
      "learning_rate": 0.0005384615384615385,
      "loss": 0.1364,
      "step": 875
    },
    {
      "epoch": 4.512820512820513,
      "grad_norm": 0.05485517904162407,
      "learning_rate": 0.0005128205128205128,
      "loss": 0.0882,
      "step": 880
    },
    {
      "epoch": 4.538461538461538,
      "grad_norm": 0.0468745119869709,
      "learning_rate": 0.0004871794871794872,
      "loss": 0.0844,
      "step": 885
    },
    {
      "epoch": 4.564102564102564,
      "grad_norm": 0.04150307923555374,
      "learning_rate": 0.0004615384615384616,
      "loss": 0.0633,
      "step": 890
    },
    {
      "epoch": 4.589743589743589,
      "grad_norm": 0.12626086175441742,
      "learning_rate": 0.0004358974358974359,
      "loss": 0.0721,
      "step": 895
    },
    {
      "epoch": 4.615384615384615,
      "grad_norm": 0.23444604873657227,
      "learning_rate": 0.0004102564102564103,
      "loss": 0.1013,
      "step": 900
    },
    {
      "epoch": 4.641025641025641,
      "grad_norm": 0.2358182966709137,
      "learning_rate": 0.00038461538461538467,
      "loss": 0.047,
      "step": 905
    },
    {
      "epoch": 4.666666666666667,
      "grad_norm": 0.13601575791835785,
      "learning_rate": 0.00035897435897435894,
      "loss": 0.1014,
      "step": 910
    },
    {
      "epoch": 4.6923076923076925,
      "grad_norm": 0.2887444794178009,
      "learning_rate": 0.0003333333333333333,
      "loss": 0.1129,
      "step": 915
    },
    {
      "epoch": 4.717948717948718,
      "grad_norm": 0.4036451578140259,
      "learning_rate": 0.0003076923076923077,
      "loss": 0.1174,
      "step": 920
    },
    {
      "epoch": 4.743589743589744,
      "grad_norm": 0.14720052480697632,
      "learning_rate": 0.0002820512820512821,
      "loss": 0.0342,
      "step": 925
    },
    {
      "epoch": 4.769230769230769,
      "grad_norm": 0.4101139008998871,
      "learning_rate": 0.0002564102564102564,
      "loss": 0.0979,
      "step": 930
    },
    {
      "epoch": 4.794871794871795,
      "grad_norm": 0.2955555319786072,
      "learning_rate": 0.0002307692307692308,
      "loss": 0.1654,
      "step": 935
    },
    {
      "epoch": 4.82051282051282,
      "grad_norm": 0.049501243978738785,
      "learning_rate": 0.00020512820512820514,
      "loss": 0.1238,
      "step": 940
    },
    {
      "epoch": 4.846153846153846,
      "grad_norm": 0.23248201608657837,
      "learning_rate": 0.00017948717948717947,
      "loss": 0.0422,
      "step": 945
    },
    {
      "epoch": 4.871794871794872,
      "grad_norm": 0.21623189747333527,
      "learning_rate": 0.00015384615384615385,
      "loss": 0.108,
      "step": 950
    },
    {
      "epoch": 4.897435897435898,
      "grad_norm": 0.03732261806726456,
      "learning_rate": 0.0001282051282051282,
      "loss": 0.0768,
      "step": 955
    },
    {
      "epoch": 4.923076923076923,
      "grad_norm": 0.05655371770262718,
      "learning_rate": 0.00010256410256410257,
      "loss": 0.1489,
      "step": 960
    },
    {
      "epoch": 4.948717948717949,
      "grad_norm": 0.03590615838766098,
      "learning_rate": 7.692307692307693e-05,
      "loss": 0.1075,
      "step": 965
    },
    {
      "epoch": 4.9743589743589745,
      "grad_norm": 0.1878024935722351,
      "learning_rate": 5.1282051282051286e-05,
      "loss": 0.1322,
      "step": 970
    },
    {
      "epoch": 5.0,
      "grad_norm": 0.19202658534049988,
      "learning_rate": 2.5641025641025643e-05,
      "loss": 0.0713,
      "step": 975
    },
    {
      "epoch": 5.0,
      "eval_loss": 0.19082452356815338,
      "eval_runtime": 0.9683,
      "eval_samples_per_second": 45.442,
      "eval_steps_per_second": 45.442,
      "step": 975
    }
  ],
  "logging_steps": 5,
  "max_steps": 975,
  "num_input_tokens_seen": 0,
  "num_train_epochs": 5,
  "save_steps": 500,
  "stateful_callbacks": {
    "TrainerControl": {
      "args": {
        "should_epoch_stop": false,
        "should_evaluate": false,
        "should_log": false,
        "should_save": true,
        "should_training_stop": true
      },
      "attributes": {}
    }
  },
  "total_flos": 665461171814400.0,
  "train_batch_size": 1,
  "trial_name": null,
  "trial_params": null
}
